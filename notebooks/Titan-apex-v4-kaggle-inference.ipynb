{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ba79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "!pip install -q albumentations timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633386b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Offline timm Check\n",
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "    import timm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ccb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - UPDATE WEIGHTS_PATH TO YOUR MODEL\n",
    "# ============================================================\n",
    "class CFG:\n",
    "    # KAGGLE PATHS\n",
    "    TEST_DIR = '/kaggle/input/recodai-luc-scientific-image-forgery-detection/test_images'\n",
    "    SAMPLE_SUB = '/kaggle/input/recodai-luc-scientific-image-forgery-detection/sample_submission.csv'\n",
    "    \n",
    "    # !!! UPDATE THIS PATH TO YOUR UPLOADED WEIGHTS !!!\n",
    "    WEIGHTS_PATH = '/kaggle/input/titan-apex-v4/TITAN_APEX_V4_BEST.pth'\n",
    "    \n",
    "    # Model settings (MUST match training)\n",
    "    ENCODER = 'efficientnet_b4'\n",
    "    IMG_SIZE = 512\n",
    "    \n",
    "    # TTA settings - Multi-scale for robustness\n",
    "    TTA_SCALES = [480, 512, 544]  # 3 scales\n",
    "    USE_FLIP_TTA = True  # +horizontal +vertical = 9 predictions total\n",
    "    \n",
    "    # Thresholding - validated at 0.50 for F1=0.5076\n",
    "    THRESHOLD = 0.50\n",
    "    \n",
    "    # Post-processing\n",
    "    MIN_AREA = 100  # Minimum pixels to keep\n",
    "    MORPH_KERNEL = 3\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Config loaded. Device: {CFG.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a89b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SRM FILTERS - 30 HIGH-PASS KERNELS FOR FORENSICS\n",
    "# ============================================================\n",
    "def get_srm_kernels():\n",
    "    \"\"\"30 SRM high-pass filter kernels for forensic analysis\"\"\"\n",
    "    kernels = []\n",
    "    \n",
    "    # 1st order edge detectors (4 kernels)\n",
    "    edge1 = np.array([[0, 0, 0], [0, -1, 1], [0, 0, 0]], dtype=np.float32)\n",
    "    for _ in range(4):\n",
    "        kernels.append(edge1.copy())\n",
    "        edge1 = np.rot90(edge1)\n",
    "    \n",
    "    # 2nd order edge detectors (4 kernels)\n",
    "    edge2 = np.array([[0, 0, 0], [1, -2, 1], [0, 0, 0]], dtype=np.float32)\n",
    "    for _ in range(4):\n",
    "        kernels.append(edge2.copy())\n",
    "        edge2 = np.rot90(edge2)\n",
    "    \n",
    "    # 3rd order SQUARE kernels (4 kernels)\n",
    "    square3 = np.array([[0, 0, 0], [-1, 3, -3], [0, 0, 1]], dtype=np.float32)\n",
    "    for _ in range(4):\n",
    "        kernels.append(square3.copy())\n",
    "        square3 = np.rot90(square3)\n",
    "    \n",
    "    # 3rd order EDGE kernels (4 kernels)\n",
    "    edge3 = np.array([[0, 0, 0], [-1, 2, -1], [0, 0, 0]], dtype=np.float32)\n",
    "    edge3[2, 1] = 0\n",
    "    edge3[1, 0] = 1\n",
    "    for _ in range(4):\n",
    "        kernels.append(edge3.copy())\n",
    "        edge3 = np.rot90(edge3)\n",
    "    \n",
    "    # SQUARE 3x3 (1 kernel)\n",
    "    square = np.array([[-1, 2, -1], [2, -4, 2], [-1, 2, -1]], dtype=np.float32)\n",
    "    kernels.append(square)\n",
    "    \n",
    "    # SQUARE 5x5 center\n",
    "    sq5 = np.array([[-1, 2, -2, 2, -1],\n",
    "                    [2, -6, 8, -6, 2],\n",
    "                    [-2, 8, -12, 8, -2],\n",
    "                    [2, -6, 8, -6, 2],\n",
    "                    [-1, 2, -2, 2, -1]], dtype=np.float32) / 12.0\n",
    "    kernels.append(sq5[1:4, 1:4].copy())\n",
    "    \n",
    "    # Additional high-pass filters\n",
    "    hp1 = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]], dtype=np.float32)\n",
    "    kernels.append(hp1)\n",
    "    \n",
    "    hp2 = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype=np.float32)\n",
    "    kernels.append(hp2)\n",
    "    \n",
    "    # Diagonal filters (4 kernels)\n",
    "    diag1 = np.array([[0, 0, 1], [0, -2, 0], [1, 0, 0]], dtype=np.float32)\n",
    "    for _ in range(4):\n",
    "        kernels.append(diag1.copy())\n",
    "        diag1 = np.rot90(diag1)\n",
    "    \n",
    "    # Additional edge variants (4 kernels)\n",
    "    ev1 = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]], dtype=np.float32) / 4\n",
    "    for _ in range(4):\n",
    "        kernels.append(ev1.copy())\n",
    "        ev1 = np.rot90(ev1)\n",
    "    \n",
    "    # Ensure exactly 30 kernels\n",
    "    while len(kernels) < 30:\n",
    "        k = np.random.randn(3, 3).astype(np.float32)\n",
    "        k = k - k.mean()\n",
    "        kernels.append(k)\n",
    "    \n",
    "    kernels = kernels[:30]\n",
    "    kernels = np.stack(kernels)\n",
    "    kernels = kernels[:, np.newaxis, :, :]\n",
    "    \n",
    "    return kernels\n",
    "\n",
    "\n",
    "class SRMConv2d(nn.Module):\n",
    "    \"\"\"SRM layer with 30 fixed high-pass filters\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        kernels = get_srm_kernels()\n",
    "        kernels = np.tile(kernels, (1, 3, 1, 1))\n",
    "        self.register_buffer('weight', torch.from_numpy(kernels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.conv2d(x, self.weight, padding=1)\n",
    "        out = torch.clamp(out, -3, 3)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BayarConv2d(nn.Module):\n",
    "    \"\"\"Bayar constrained convolution\"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel = nn.Parameter(torch.randn(out_channels, in_channels, 5, 5) * 0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        kernel = self.kernel.clone()\n",
    "        center_mask = torch.zeros(5, 5, device=kernel.device)\n",
    "        center_mask[2, 2] = 1\n",
    "        \n",
    "        non_center = kernel * (1 - center_mask)\n",
    "        non_center_sum = non_center.sum(dim=(2, 3), keepdim=True) + 1e-8\n",
    "        non_center = non_center / non_center_sum\n",
    "        \n",
    "        constrained = non_center\n",
    "        constrained[:, :, 2, 2] = -1\n",
    "        \n",
    "        return F.conv2d(x, constrained, padding=2)\n",
    "\n",
    "\n",
    "def compute_physics_maps(x):\n",
    "    \"\"\"Compute physics-based forensic features\"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    gray = x.mean(dim=1, keepdim=True)\n",
    "    \n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "                           dtype=x.dtype, device=x.device).view(1, 1, 3, 3)\n",
    "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n",
    "                           dtype=x.dtype, device=x.device).view(1, 1, 3, 3)\n",
    "    \n",
    "    gx = F.conv2d(gray, sobel_x, padding=1)\n",
    "    gy = F.conv2d(gray, sobel_y, padding=1)\n",
    "    grad_mag = torch.sqrt(gx**2 + gy**2 + 1e-8)\n",
    "    \n",
    "    mean_filter = torch.ones(1, 1, 5, 5, dtype=x.dtype, device=x.device) / 25\n",
    "    local_mean = F.conv2d(gray, mean_filter, padding=2)\n",
    "    local_sq_mean = F.conv2d(gray**2, mean_filter, padding=2)\n",
    "    local_var = torch.clamp(local_sq_mean - local_mean**2, min=0)\n",
    "    \n",
    "    blur = F.avg_pool2d(gray, 3, stride=1, padding=1)\n",
    "    noise_res = gray - blur\n",
    "    \n",
    "    laplacian = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]], \n",
    "                             dtype=x.dtype, device=x.device).view(1, 1, 3, 3)\n",
    "    edge = torch.abs(F.conv2d(gray, laplacian, padding=1))\n",
    "    \n",
    "    def normalize(t):\n",
    "        t_min = t.view(B, 1, -1).min(dim=-1, keepdim=True)[0].unsqueeze(-1)\n",
    "        t_max = t.view(B, 1, -1).max(dim=-1, keepdim=True)[0].unsqueeze(-1)\n",
    "        return (t - t_min) / (t_max - t_min + 1e-8)\n",
    "    \n",
    "    return torch.cat([\n",
    "        normalize(grad_mag),\n",
    "        normalize(local_var),\n",
    "        normalize(torch.abs(noise_res)),\n",
    "        normalize(edge)\n",
    "    ], dim=1)\n",
    "\n",
    "print(\"Forensic modules defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ATTENTION MODULES\n",
    "# ============================================================\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        w = self.fc(x).view(x.size(0), x.size(1), 1, 1)\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg = x.mean(dim=1, keepdim=True)\n",
    "        max_val = x.max(dim=1, keepdim=True)[0]\n",
    "        w = self.conv(torch.cat([avg, max_val], dim=1))\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.ca(x)\n",
    "        x = self.sa(x)\n",
    "        return x\n",
    "\n",
    "print(\"Attention modules defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc54b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENCODER - DUAL STREAM (SEMANTIC + FORENSIC)\n",
    "# ============================================================\n",
    "class DualStreamEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.semantic = timm.create_model(\n",
    "            CFG.ENCODER,\n",
    "            pretrained=False,\n",
    "            features_only=True,\n",
    "            out_indices=(0, 1, 2, 3, 4)\n",
    "        )\n",
    "        \n",
    "        dummy = torch.zeros(1, 3, CFG.IMG_SIZE, CFG.IMG_SIZE)\n",
    "        with torch.no_grad():\n",
    "            feats = self.semantic(dummy)\n",
    "            self.semantic_channels = [f.shape[1] for f in feats]\n",
    "        \n",
    "        self.srm = SRMConv2d()\n",
    "        self.bayar = BayarConv2d(3, 3)\n",
    "        \n",
    "        self.forensic_conv = nn.Sequential(\n",
    "            nn.Conv2d(37, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.fusions = nn.ModuleList()\n",
    "        for ch in self.semantic_channels:\n",
    "            self.fusions.append(nn.Sequential(\n",
    "                nn.Conv2d(ch + 64, ch, 1),\n",
    "                nn.BatchNorm2d(ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                CBAM(ch)\n",
    "            ))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        semantic_feats = self.semantic(x)\n",
    "        \n",
    "        srm_out = self.srm(x)\n",
    "        bayar_out = self.bayar(x)\n",
    "        physics = compute_physics_maps(x)\n",
    "        \n",
    "        forensic = torch.cat([srm_out, bayar_out, physics], dim=1)\n",
    "        forensic = self.forensic_conv(forensic)\n",
    "        \n",
    "        fused_feats = []\n",
    "        for i, (sem_feat, fusion) in enumerate(zip(semantic_feats, self.fusions)):\n",
    "            h, w = sem_feat.shape[2:]\n",
    "            forensic_scaled = F.interpolate(forensic, size=(h, w), mode='bilinear', align_corners=False)\n",
    "            combined = torch.cat([sem_feat, forensic_scaled], dim=1)\n",
    "            fused = fusion(combined)\n",
    "            fused_feats.append(fused)\n",
    "        \n",
    "        return fused_feats\n",
    "\n",
    "print(\"Encoder defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DECODER WITH DEEP SUPERVISION\n",
    "# ============================================================\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch + skip_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            CBAM(out_ch)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoder_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dec4 = DecoderBlock(encoder_channels[4], encoder_channels[3], 256)\n",
    "        self.dec3 = DecoderBlock(256, encoder_channels[2], 128)\n",
    "        self.dec2 = DecoderBlock(128, encoder_channels[1], 64)\n",
    "        self.dec1 = DecoderBlock(64, encoder_channels[0], 32)\n",
    "        \n",
    "        self.final_up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, 1)\n",
    "        )\n",
    "        \n",
    "        self.ds4 = nn.Conv2d(256, 1, 1)\n",
    "        self.ds3 = nn.Conv2d(128, 1, 1)\n",
    "        self.ds2 = nn.Conv2d(64, 1, 1)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        f0, f1, f2, f3, f4 = features\n",
    "        \n",
    "        d4 = self.dec4(f4, f3)\n",
    "        d3 = self.dec3(d4, f2)\n",
    "        d2 = self.dec2(d3, f1)\n",
    "        d1 = self.dec1(d2, f0)\n",
    "        \n",
    "        out = self.final_up(d1)\n",
    "        out = self.final_conv(out)\n",
    "        \n",
    "        ds4 = self.ds4(d4)\n",
    "        ds3 = self.ds3(d3)\n",
    "        ds2 = self.ds2(d2)\n",
    "        \n",
    "        return out, [ds4, ds3, ds2]\n",
    "\n",
    "\n",
    "class AuxiliaryHead(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "print(\"Decoder defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL MODEL - TITAN APEX V4\n",
    "# ============================================================\n",
    "class TitanApexV4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = DualStreamEncoder()\n",
    "        self.decoder = Decoder(self.encoder.semantic_channels)\n",
    "        self.aux_head = AuxiliaryHead(self.encoder.semantic_channels[-1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        mask, ds_outputs = self.decoder(features)\n",
    "        aux_logit = self.aux_head(features[-1])\n",
    "        \n",
    "        return {\n",
    "            'mask': mask,\n",
    "            'ds_outputs': ds_outputs,\n",
    "            'aux': aux_logit\n",
    "        }\n",
    "\n",
    "print(\"TitanApexV4 model defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "def rle_encode(mask):\n",
    "    \"\"\"Run-length encoding for submission\"\"\"\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return json.dumps([int(x) for x in runs])\n",
    "\n",
    "\n",
    "def post_process_mask(mask, min_area=100, kernel_size=3):\n",
    "    \"\"\"Post-processing to clean predictions\"\"\"\n",
    "    if mask.sum() == 0:\n",
    "        return mask\n",
    "    \n",
    "    mask = mask.astype(np.uint8)\n",
    "    \n",
    "    # Morphological closing then opening\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Remove small connected components\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
    "    \n",
    "    clean_mask = np.zeros_like(mask)\n",
    "    for i in range(1, num_labels):\n",
    "        area = stats[i, cv2.CC_STAT_AREA]\n",
    "        if area >= min_area:\n",
    "            clean_mask[labels == i] = 1\n",
    "    \n",
    "    return clean_mask\n",
    "\n",
    "\n",
    "def get_transform(img_size):\n",
    "    \"\"\"Get transform for specific image size\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================\n",
    "def load_model(path):\n",
    "    \"\"\"Load trained model weights\"\"\"\n",
    "    print(f\">>> Loading TITAN-APEX V4 Model: {path}\")\n",
    "    model = TitanApexV4()\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        state = torch.load(path, map_location=CFG.device)\n",
    "        if 'model_state_dict' in state:\n",
    "            state = state['model_state_dict']\n",
    "        model.load_state_dict(state, strict=True)\n",
    "        print(\">>> Weights Loaded Successfully.\")\n",
    "    else:\n",
    "        print(f\"!!! CRITICAL: Weights not found at {path}\")\n",
    "        raise FileNotFoundError(f\"Model weights not found: {path}\")\n",
    "    \n",
    "    model.to(CFG.device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "model = load_model(CFG.WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TTA PREDICTION FUNCTION\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def predict_with_tta(model, image, original_size):\n",
    "    \"\"\"\n",
    "    Multi-scale + flip TTA for robust predictions\n",
    "    - 3 scales × (1 base + 2 flips) = 9 predictions averaged\n",
    "    \"\"\"\n",
    "    h, w = original_size\n",
    "    all_preds = []\n",
    "    \n",
    "    for scale in CFG.TTA_SCALES:\n",
    "        transform = get_transform(scale)\n",
    "        \n",
    "        # Base prediction\n",
    "        img_t = transform(image=image)['image'].unsqueeze(0).to(CFG.device)\n",
    "        out = model(img_t)\n",
    "        pred = torch.sigmoid(out['mask'])[0, 0]\n",
    "        pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=(h, w), \n",
    "                            mode='bilinear', align_corners=False)[0, 0]\n",
    "        all_preds.append(pred)\n",
    "        \n",
    "        if CFG.USE_FLIP_TTA:\n",
    "            # Horizontal flip\n",
    "            img_hflip = np.fliplr(image).copy()\n",
    "            img_t = transform(image=img_hflip)['image'].unsqueeze(0).to(CFG.device)\n",
    "            out = model(img_t)\n",
    "            pred = torch.sigmoid(out['mask'])[0, 0]\n",
    "            pred = torch.flip(pred, [1])\n",
    "            pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=(h, w), \n",
    "                                mode='bilinear', align_corners=False)[0, 0]\n",
    "            all_preds.append(pred)\n",
    "            \n",
    "            # Vertical flip\n",
    "            img_vflip = np.flipud(image).copy()\n",
    "            img_t = transform(image=img_vflip)['image'].unsqueeze(0).to(CFG.device)\n",
    "            out = model(img_t)\n",
    "            pred = torch.sigmoid(out['mask'])[0, 0]\n",
    "            pred = torch.flip(pred, [0])\n",
    "            pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=(h, w), \n",
    "                                mode='bilinear', align_corners=False)[0, 0]\n",
    "            all_preds.append(pred)\n",
    "    \n",
    "    # Average all predictions\n",
    "    avg_pred = torch.stack(all_preds).mean(dim=0)\n",
    "    return avg_pred.cpu().numpy()\n",
    "\n",
    "print(f\"TTA will use {len(CFG.TTA_SCALES)} scales × 3 augmentations = {len(CFG.TTA_SCALES) * 3} predictions per image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba46fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN INFERENCE\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TITAN-APEX V4 INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {CFG.device}\")\n",
    "print(f\"TTA Scales: {CFG.TTA_SCALES}\")\n",
    "print(f\"Flip TTA: {CFG.USE_FLIP_TTA}\")\n",
    "print(f\"Threshold: {CFG.THRESHOLD}\")\n",
    "print(f\"Min Area: {CFG.MIN_AREA}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find all test images\n",
    "all_files = glob.glob(os.path.join(CFG.TEST_DIR, '**', '*'), recursive=True)\n",
    "id_map = {}\n",
    "for f in all_files:\n",
    "    ext = os.path.splitext(f)[1].lower()\n",
    "    if ext in ['.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp']:\n",
    "        base = os.path.basename(f)\n",
    "        digits = ''.join(filter(str.isdigit, os.path.splitext(base)[0]))\n",
    "        if digits:\n",
    "            id_map[str(int(digits))] = f\n",
    "\n",
    "print(f\">>> Found {len(id_map)} test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e718853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all images\n",
    "preds_list = []\n",
    "\n",
    "for case_id, path in tqdm(id_map.items(), desc=\"Processing\"):\n",
    "    label = \"authentic\"\n",
    "    \n",
    "    try:\n",
    "        # Load image\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            image = np.array(Image.open(path).convert('RGB'))\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Predict with TTA\n",
    "        prob_map = predict_with_tta(model, image, (h, w))\n",
    "        \n",
    "        # Threshold\n",
    "        binary_mask = (prob_map >= CFG.THRESHOLD).astype(np.uint8)\n",
    "        \n",
    "        # Post-process\n",
    "        clean_mask = post_process_mask(binary_mask, CFG.MIN_AREA, CFG.MORPH_KERNEL)\n",
    "        \n",
    "        # Encode if forgery detected\n",
    "        if clean_mask.sum() > 0:\n",
    "            label = rle_encode(clean_mask)\n",
    "            if label == \"\" or label == \"[]\":\n",
    "                label = \"authentic\"\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {case_id}: {e}\")\n",
    "        label = \"authentic\"\n",
    "    \n",
    "    preds_list.append({\"case_id\": case_id, \"annotation\": label})\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n>>> Processed {len(preds_list)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE SUBMISSION\n",
    "# ============================================================\n",
    "try:\n",
    "    sample_sub = pd.read_csv(CFG.SAMPLE_SUB)\n",
    "except:\n",
    "    sample_sub = pd.DataFrame({'case_id': list(id_map.keys()), 'annotation': ['authentic'] * len(id_map)})\n",
    "\n",
    "sample_sub['case_id'] = sample_sub['case_id'].astype(str)\n",
    "\n",
    "if len(preds_list) > 0:\n",
    "    preds_df = pd.DataFrame(preds_list)\n",
    "    preds_df['case_id'] = preds_df['case_id'].astype(str)\n",
    "    submission = sample_sub[['case_id']].merge(preds_df, on='case_id', how='left')\n",
    "    submission['annotation'] = submission['annotation'].fillna(\"authentic\")\n",
    "else:\n",
    "    submission = sample_sub\n",
    "    submission['annotation'] = 'authentic'\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Stats\n",
    "forged_count = (submission['annotation'] != 'authentic').sum()\n",
    "print(\"=\" * 60)\n",
    "print(f\">>> SUCCESS: Submission saved!\")\n",
    "print(f\">>> Total images: {len(submission)}\")\n",
    "print(f\">>> Forged detected: {forged_count}\")\n",
    "print(f\">>> Authentic: {len(submission) - forged_count}\")\n",
    "print(\"=\" * 60)\n",
    "print(submission.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
